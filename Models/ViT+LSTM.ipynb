{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# prompt: mount\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yweOU3Ec8_6V",
        "outputId": "3821baa4-bc0d-436d-ca1a-8c53527893e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imagecodecs --quiet\n",
        "!pip install timm torchvision --quiet"
      ],
      "metadata": {
        "id": "KFjebN6I6H1X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe7cb6b-ca34-4750-ff20-d52a7b048a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 ▶ Consolidated imports\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "\n",
        "from tifffile import imread\n"
      ],
      "metadata": {
        "id": "ptMN1vOt6I_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1 ▶ Dataset (224×224 targets, no down-scaling + 6-h sequence)\n",
        "class LSTDataset(Dataset):\n",
        "    def __init__(self, df, patches_dir, weather_cols, n_hours=6):\n",
        "        self.df           = df.reset_index(drop=True)\n",
        "        self.patches_dir  = patches_dir\n",
        "        self.weather_cols = weather_cols\n",
        "        self.n_hours      = n_hours\n",
        "        self.n_vars       = len(weather_cols) // n_hours\n",
        "        self.transform    = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224,224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.485,0.456,0.406],\n",
        "                std =[0.229,0.224,0.225],\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row  = self.df.loc[idx]\n",
        "        arr  = imread(os.path.join(self.patches_dir, row[\"patch_filename\"])\n",
        "                     ).astype(np.float32)            # (4,H,W)\n",
        "\n",
        "        # ── image input ───────────────────────────────────\n",
        "        img_np = arr[[1,2,3]].transpose(1,2,0).astype(np.uint8)\n",
        "        img    = self.transform(img_np)              # [3,224,224]\n",
        "\n",
        "        # ── target LST ───────────────────────────────────\n",
        "        tar_np = arr[0]                              # (H,W)\n",
        "        tar    = torch.tensor(tar_np, dtype=torch.float32).unsqueeze(0)\n",
        "        tar    = F.interpolate(\n",
        "            tar.unsqueeze(0),\n",
        "            size=(224,224),\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        ).squeeze(0)                                 # [1,224,224]\n",
        "\n",
        "        # ── meteorological sequence [6×5] ───────────────\n",
        "        w_flat = row[self.weather_cols].values.astype(np.float32)\n",
        "        w_seq  = torch.from_numpy(w_flat).view(self.n_hours, self.n_vars)\n",
        "\n",
        "        return img, w_seq, tar\n"
      ],
      "metadata": {
        "id": "cElldHbx6U2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 ▶ Load merged patch+6h-meteo CSV & build DataLoaders\n",
        "csv_path    = \"/content/drive/MyDrive/patch_with_meteo_last6h.csv\"\n",
        "patches_dir = \"/content/drive/MyDrive/PatchedOutput_Cleaned\"\n",
        "\n",
        "# 1) read & drop truly missing\n",
        "df = pd.read_csv(csv_path, parse_dates=['date'])\n",
        "df = df.dropna(subset=['patch_filename','date']).reset_index(drop=True)\n",
        "\n",
        "# 2) pick exactly the 6h-lag columns\n",
        "seq_cols = [c for c in df.columns if \"_t-\" in c]\n",
        "assert len(seq_cols) == 6*5, f\"expected 30 lag cols, got {len(seq_cols)}\"\n",
        "\n",
        "print(\"Using 6-hour sequence cols:\", seq_cols[:5], \"... total =\", len(seq_cols))\n",
        "\n",
        "# 3) build dataset with those only\n",
        "dataset = LSTDataset(df, patches_dir, seq_cols, n_hours=6)\n",
        "\n",
        "# 4) split & loaders\n",
        "n_train = int(0.8 * len(dataset))\n",
        "train_ds, val_ds = random_split(dataset, [n_train, len(dataset)-n_train])\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True,\n",
        "                          num_workers=0, pin_memory=False)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False,\n",
        "                          num_workers=0, pin_memory=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O1vNrXl6We9",
        "outputId": "bcdfa8de-0e93-4ae1-8308-db0ffc1fa990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 6-hour sequence cols: ['air_temp_C_t-5h', 'dew_point_C_t-5h', 'relative_humidity_percent_t-5h', 'wind_speed_m_s_t-5h', 'precipitation_in_t-5h'] ... total = 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 ▶ ViT + LSTM fusion → 224×224 decoder\n",
        "class PretrainedViTLSTModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 weather_dim=5,      # vars per time step\n",
        "                 hidden_dim=768,\n",
        "                 vit_name=\"vit_base_patch16_224\",\n",
        "                 lstm_layers=1,\n",
        "                 lstm_dropout=0.1,\n",
        "                 num_transformer_layers=2,\n",
        "                 num_heads=8):\n",
        "        super().__init__()\n",
        "        # ViT backbone\n",
        "        self.vit = timm.create_model(vit_name, pretrained=True, num_classes=0)\n",
        "        for p in self.vit.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # LSTM for weather sequence → one embedding\n",
        "        self.weather_encoder = nn.LSTM(\n",
        "            input_size=weather_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=lstm_dropout if lstm_layers > 1 else 0.0\n",
        "        )\n",
        "\n",
        "        # fusion transformer\n",
        "        enc = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim * 4,\n",
        "            dropout=0.1\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(enc, num_transformer_layers)\n",
        "\n",
        "        # decoder up-sample 14→28→56→112→224\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(hidden_dim,   hidden_dim // 2, 2, 2),\n",
        "            nn.BatchNorm2d(hidden_dim // 2), nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(hidden_dim // 2, hidden_dim // 4, 2, 2),\n",
        "            nn.BatchNorm2d(hidden_dim // 4), nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(hidden_dim // 4, hidden_dim // 8, 2, 2),\n",
        "            nn.BatchNorm2d(hidden_dim // 8), nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(hidden_dim // 8,      1,         2, 2),\n",
        "        )\n",
        "\n",
        "    def forward(self, images, weather_seq):\n",
        "        # ViT → [B,197,768]\n",
        "        feats   = self.vit.forward_features(images)\n",
        "        cls_tok = feats[:, :1]       # [B,1,768]\n",
        "        patch_t = feats[:, 1:]       # [B,196,768]\n",
        "\n",
        "        # LSTM → (outputs, (h_n, c_n)); take last layer's h_n\n",
        "        _, (h_n, _) = self.weather_encoder(weather_seq)\n",
        "        w_tok = h_n[-1].unsqueeze(1) # [B,1,hidden_dim]\n",
        "\n",
        "        # concat → [B,198,hidden_dim]\n",
        "        tokens = torch.cat([patch_t, w_tok, cls_tok], dim=1)\n",
        "\n",
        "        # transformer expects [seq, batch, dim]\n",
        "        t = self.transformer(tokens.permute(1, 0, 2)).permute(1, 0, 2)\n",
        "        patch_out = t[:, :-2, :]     # drop weather + CLS → [B,196,dim]\n",
        "\n",
        "        # reshape 196 → 14×14\n",
        "        B, N, D = patch_out.size()\n",
        "        G = int(math.sqrt(N))\n",
        "        x = patch_out.transpose(1, 2).view(B, D, G, G)\n",
        "\n",
        "        return self.deconv(x)        # [B,1,224,224]\n"
      ],
      "metadata": {
        "id": "YHvVAvaR6Wbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5 ▶ Instantiate & unfreeze last ViT layers\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = PretrainedViTLSTModel(\n",
        "    weather_dim            = len(seq_cols) // 6,   # seq_cols was defined in Cell 3\n",
        "    hidden_dim             = 768,\n",
        "    vit_name               = \"vit_base_patch16_224\",\n",
        "    lstm_layers            = 1,                   # keep as desired\n",
        "    lstm_dropout           = 0.1,\n",
        "    num_transformer_layers = 2,                   # matches __init__ signature\n",
        "    num_heads              = 8\n",
        ").to(device)\n",
        "\n",
        "# unfreeze final ViT blocks\n",
        "for name, p in model.vit.named_parameters():\n",
        "    if any(layer in name for layer in [\"blocks.10\", \"blocks.11\", \"norm\"]):\n",
        "        p.requires_grad = True\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLpQf1yK7sg3",
        "outputId": "8c779026-be84-4eaa-a2bc-2f77ef509a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6 ▶ Optimizer, loss & scheduler\n",
        "opt       = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=1e-4, weight_decay=1e-2\n",
        ")\n",
        "loss_fn   = nn.SmoothL1Loss()\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    opt, mode='min', factor=0.5, patience=3, verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm8JkAy7ppo-",
        "outputId": "2d170722-1997-4d5c-dfd1-6de081bfed83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import math\n",
        "\n",
        "# — optionally re-init your model, optimizer & scheduler here —\n",
        "# model     = ViT_LSTM(...).to(device)\n",
        "# opt       = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=5)\n",
        "\n",
        "num_epochs = 20\n",
        "save_dir = Path(\"/content/drive/MyDrive/Model_vit_lstm_Checkpoints\")\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # —— TRAIN ——\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    seen       = 0\n",
        "    train_bar  = tqdm(train_loader, desc=f\"Epoch {epoch+1:02d} Train\", unit=\"batch\")\n",
        "    for imgs, w_seq, tgt in train_bar:\n",
        "        imgs, w_seq, tgt = imgs.to(device), w_seq.to(device), tgt.to(device)\n",
        "        opt.zero_grad()\n",
        "        out      = model(imgs, w_seq)\n",
        "        loss     = loss_fn(out, tgt)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        bsz = imgs.size(0)\n",
        "        train_loss += loss.item() * bsz\n",
        "        seen      += bsz\n",
        "        train_bar.set_postfix(\n",
        "            batch_loss=f\"{loss.item():.4f}\",\n",
        "            avg_loss  =f\"{train_loss/seen:.4f}\"\n",
        "        )\n",
        "\n",
        "    train_rmse = math.sqrt(train_loss / len(train_loader.dataset))\n",
        "\n",
        "    # —— VALIDATE ——\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    seen     = 0\n",
        "    val_bar  = tqdm(val_loader, desc=f\"Epoch {epoch+1:02d}   Val\", unit=\"batch\")\n",
        "    with torch.no_grad():\n",
        "        for imgs, w_seq, tgt in val_bar:\n",
        "            imgs, w_seq, tgt = imgs.to(device), w_seq.to(device), tgt.to(device)\n",
        "            out      = model(imgs, w_seq)\n",
        "            l_val    = loss_fn(out, tgt).item()\n",
        "            bsz      = imgs.size(0)\n",
        "            val_loss += l_val * bsz\n",
        "            seen     += bsz\n",
        "            val_bar.set_postfix(\n",
        "                batch_loss=f\"{l_val:.4f}\",\n",
        "                avg_loss  =f\"{val_loss/seen:.4f}\"\n",
        "            )\n",
        "\n",
        "    val_rmse = math.sqrt(val_loss / len(val_loader.dataset))\n",
        "    scheduler.step(val_loss)   # or scheduler.step() if epoch‐based\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} ▶ Train RMSE: {train_rmse:.3f} | Val RMSE: {val_rmse:.3f}\")\n",
        "\n",
        "    # —— SAVE CHECKPOINT ——\n",
        "    ckpt = {\n",
        "        'epoch': epoch+1,\n",
        "        'model_state_dict':      model.state_dict(),\n",
        "        'optimizer_state_dict':  opt.state_dict(),\n",
        "        'scheduler_state_dict':  scheduler.state_dict(),\n",
        "        'train_rmse':            train_rmse,\n",
        "        'val_rmse':              val_rmse\n",
        "    }\n",
        "    torch.save(ckpt, save_dir/f\"vit_lstm_epoch{epoch+1:02d}.pt\")\n",
        "    print(\"✅ Saved checkpoint.\")\n",
        "\n",
        "print(\"✅ Training complete (0 → 20 epochs)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1_AfmCqBUtz",
        "outputId": "0788a3fe-3392-425e-d422-fe9179480af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 01 Train: 100%|██████████| 2295/2295 [02:34<00:00, 14.82batch/s, avg_loss=1.3455, batch_loss=0.3028]\n",
            "Epoch 01   Val: 100%|██████████| 574/574 [00:28<00:00, 20.13batch/s, avg_loss=0.9677, batch_loss=0.8528]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 ▶ Train RMSE: 1.160 | Val RMSE: 0.984\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 02 Train: 100%|██████████| 2295/2295 [02:49<00:00, 13.56batch/s, avg_loss=0.5450, batch_loss=0.3008]\n",
            "Epoch 02   Val: 100%|██████████| 574/574 [00:28<00:00, 20.27batch/s, avg_loss=0.4168, batch_loss=0.0811]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02 ▶ Train RMSE: 0.738 | Val RMSE: 0.646\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 03 Train: 100%|██████████| 2295/2295 [02:50<00:00, 13.46batch/s, avg_loss=0.3595, batch_loss=0.4615]\n",
            "Epoch 03   Val: 100%|██████████| 574/574 [00:28<00:00, 20.19batch/s, avg_loss=0.2523, batch_loss=0.1417]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03 ▶ Train RMSE: 0.600 | Val RMSE: 0.502\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 04 Train: 100%|██████████| 2295/2295 [02:49<00:00, 13.52batch/s, avg_loss=0.2902, batch_loss=0.1785]\n",
            "Epoch 04   Val: 100%|██████████| 574/574 [00:28<00:00, 19.98batch/s, avg_loss=0.2330, batch_loss=0.0539]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04 ▶ Train RMSE: 0.539 | Val RMSE: 0.483\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 05 Train: 100%|██████████| 2295/2295 [02:49<00:00, 13.51batch/s, avg_loss=0.2472, batch_loss=0.8123]\n",
            "Epoch 05   Val: 100%|██████████| 574/574 [00:28<00:00, 20.11batch/s, avg_loss=0.1844, batch_loss=0.0668]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05 ▶ Train RMSE: 0.497 | Val RMSE: 0.429\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 06 Train: 100%|██████████| 2295/2295 [02:49<00:00, 13.50batch/s, avg_loss=0.2134, batch_loss=0.0727]\n",
            "Epoch 06   Val: 100%|██████████| 574/574 [00:28<00:00, 20.15batch/s, avg_loss=0.1463, batch_loss=0.0511]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06 ▶ Train RMSE: 0.462 | Val RMSE: 0.383\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 07 Train: 100%|██████████| 2295/2295 [02:49<00:00, 13.52batch/s, avg_loss=0.1871, batch_loss=0.0211]\n",
            "Epoch 07   Val: 100%|██████████| 574/574 [00:28<00:00, 20.20batch/s, avg_loss=0.2056, batch_loss=0.0316]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07 ▶ Train RMSE: 0.433 | Val RMSE: 0.453\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 08 Train: 100%|██████████| 2295/2295 [02:50<00:00, 13.43batch/s, avg_loss=0.1817, batch_loss=0.3791]\n",
            "Epoch 08   Val: 100%|██████████| 574/574 [00:28<00:00, 19.98batch/s, avg_loss=0.1513, batch_loss=0.0412]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08 ▶ Train RMSE: 0.426 | Val RMSE: 0.389\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 09 Train: 100%|██████████| 2295/2295 [02:50<00:00, 13.48batch/s, avg_loss=0.1594, batch_loss=0.0447]\n",
            "Epoch 09   Val: 100%|██████████| 574/574 [00:28<00:00, 20.16batch/s, avg_loss=0.1389, batch_loss=0.0610]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09 ▶ Train RMSE: 0.399 | Val RMSE: 0.373\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Train: 100%|██████████| 2295/2295 [02:49<00:00, 13.52batch/s, avg_loss=0.1603, batch_loss=0.0541]\n",
            "Epoch 10   Val: 100%|██████████| 574/574 [00:28<00:00, 20.07batch/s, avg_loss=0.1247, batch_loss=0.0363]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 ▶ Train RMSE: 0.400 | Val RMSE: 0.353\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11 Train: 100%|██████████| 2295/2295 [02:49<00:00, 13.51batch/s, avg_loss=0.1411, batch_loss=0.2063]\n",
            "Epoch 11   Val: 100%|██████████| 574/574 [00:28<00:00, 19.97batch/s, avg_loss=0.1437, batch_loss=0.0359]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 ▶ Train RMSE: 0.376 | Val RMSE: 0.379\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12 Train: 100%|██████████| 2295/2295 [02:37<00:00, 14.61batch/s, avg_loss=0.1378, batch_loss=0.0435]\n",
            "Epoch 12   Val: 100%|██████████| 574/574 [00:28<00:00, 20.09batch/s, avg_loss=0.1154, batch_loss=0.0448]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 ▶ Train RMSE: 0.371 | Val RMSE: 0.340\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13 Train: 100%|██████████| 2295/2295 [02:37<00:00, 14.58batch/s, avg_loss=0.1310, batch_loss=0.0814]\n",
            "Epoch 13   Val: 100%|██████████| 574/574 [00:28<00:00, 20.10batch/s, avg_loss=0.1078, batch_loss=0.0333]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 ▶ Train RMSE: 0.362 | Val RMSE: 0.328\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14 Train: 100%|██████████| 2295/2295 [02:37<00:00, 14.56batch/s, avg_loss=0.1243, batch_loss=0.0346]\n",
            "Epoch 14   Val: 100%|██████████| 574/574 [00:28<00:00, 20.14batch/s, avg_loss=0.0932, batch_loss=0.0388]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 ▶ Train RMSE: 0.353 | Val RMSE: 0.305\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15 Train: 100%|██████████| 2295/2295 [02:37<00:00, 14.59batch/s, avg_loss=0.1176, batch_loss=0.0403]\n",
            "Epoch 15   Val: 100%|██████████| 574/574 [00:28<00:00, 20.16batch/s, avg_loss=0.0980, batch_loss=0.0298]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 ▶ Train RMSE: 0.343 | Val RMSE: 0.313\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16 Train: 100%|██████████| 2295/2295 [02:37<00:00, 14.58batch/s, avg_loss=0.1190, batch_loss=0.0167]\n",
            "Epoch 16   Val: 100%|██████████| 574/574 [00:29<00:00, 19.78batch/s, avg_loss=0.0765, batch_loss=0.0405]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 ▶ Train RMSE: 0.345 | Val RMSE: 0.277\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17 Train: 100%|██████████| 2295/2295 [02:39<00:00, 14.37batch/s, avg_loss=0.1113, batch_loss=0.0169]\n",
            "Epoch 17   Val: 100%|██████████| 574/574 [00:28<00:00, 19.84batch/s, avg_loss=0.1145, batch_loss=0.0315]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 ▶ Train RMSE: 0.334 | Val RMSE: 0.338\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18 Train: 100%|██████████| 2295/2295 [02:36<00:00, 14.64batch/s, avg_loss=0.1063, batch_loss=0.0387]\n",
            "Epoch 18   Val: 100%|██████████| 574/574 [00:28<00:00, 20.15batch/s, avg_loss=0.0954, batch_loss=0.0418]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 ▶ Train RMSE: 0.326 | Val RMSE: 0.309\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19 Train: 100%|██████████| 2295/2295 [02:36<00:00, 14.63batch/s, avg_loss=0.1004, batch_loss=3.9148]\n",
            "Epoch 19   Val: 100%|██████████| 574/574 [00:28<00:00, 20.05batch/s, avg_loss=0.1944, batch_loss=0.0678]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 ▶ Train RMSE: 0.317 | Val RMSE: 0.441\n",
            "✅ Saved checkpoint.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20 Train: 100%|██████████| 2295/2295 [02:37<00:00, 14.62batch/s, avg_loss=0.1015, batch_loss=0.0807]\n",
            "Epoch 20   Val: 100%|██████████| 574/574 [00:28<00:00, 20.18batch/s, avg_loss=0.0864, batch_loss=0.0289]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 ▶ Train RMSE: 0.319 | Val RMSE: 0.294\n",
            "✅ Saved checkpoint.\n",
            "✅ Training complete (0 → 20 epochs)\n"
          ]
        }
      ]
    }
  ]
}
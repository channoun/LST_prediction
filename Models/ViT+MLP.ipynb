{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE9t24Rweqmy",
        "outputId": "c196f063-b476-4a82-ccd3-f7b97a3ca484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAqBIFd8jKNK",
        "outputId": "fdf6f215-dedc-4ac2-9ed3-a3eecbb8776b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.1.31)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.1.8)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.0.2)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.3)\n",
            "Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.4.3\n",
            "Total samples: 18813\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 2) Imports\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "# Install rasterio if needed\n",
        "!pip install rasterio\n",
        "import rasterio  # for reading GeoTIFF patches\n",
        "\n",
        "# 3) Define paths\n",
        "# Replace 'PatchedOut' with the name of the folder/shortcut in your My Drive\n",
        "base_path      = \"/content/drive/MyDrive/PatchedOut\"\n",
        "patches_dir = os.path.join(base_path, \"PatchedOutput\")\n",
        "meta_csv_path  = \"/content/drive/MyDrive/PatchedOut/PatchedOutput/tiff_with_meteo.csv\"\n",
        "\n",
        "# 4) Load the patch metadata (which already includes your 10 AM weather features)\n",
        "# Reload CSV carefully and rename column\n",
        "patch_meta = pd.read_csv(meta_csv_path)\n",
        "patch_meta.rename(columns={'patch_file':'file_name'}, inplace=True)\n",
        "\n",
        "# Force all weather columns to numeric immediately\n",
        "weather_feature_cols = [\n",
        "    'air_temp_C',\n",
        "    'dew_point_C',\n",
        "    'relative_humidity_percent',\n",
        "    'wind_speed_m_s',\n",
        "    'precipitation_in'\n",
        "]\n",
        "\n",
        "for col in weather_feature_cols:\n",
        "    patch_meta[col] = pd.to_numeric(patch_meta[col], errors='coerce').fillna(0.0)\n",
        "\n",
        "# Also, explicitly ensure the date column is parsed correctly\n",
        "patch_meta['date'] = pd.to_datetime(patch_meta['date'], errors='coerce').dt.date\n",
        "\n",
        "# Now, explicitly create merged_df as a safe copy\n",
        "merged_df = patch_meta.copy()\n",
        "\n",
        "print(\"Total samples:\", len(merged_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynY0MEOpphc7",
        "outputId": "79aeaef3-7ef4-4131-a5af-d28d5215d1d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All columns: ['filename', 'date', 'datetime', 'air_temp_C', 'dew_point_C', 'relative_humidity_percent', 'wind_speed_m_s', 'precipitation_in']\n"
          ]
        }
      ],
      "source": [
        "patch_meta = pd.read_csv(meta_csv_path)\n",
        "print(\"All columns:\", patch_meta.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjM0ZjuSIZvX"
      },
      "outputs": [],
      "source": [
        "# Approximate means/stds based on known band characteristics\n",
        "mean_image = np.array([0.0, 0.0, 0.0])   # reasonable midpoint\n",
        "std_image = np.array([0.3, 0.3, 0.3])    # approximate std for NDVI/NDBI/NDWI\n",
        "\n",
        "# Approximate weather stats (use domain knowledge or a small subset calculation)\n",
        "mean_weather = np.zeros(len(weather_feature_cols))\n",
        "std_weather = np.ones(len(weather_feature_cols))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTHHZ9zbodLA"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import rasterio\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class LSTDataset(Dataset):\n",
        "  def __init__(\n",
        "        self,\n",
        "        data_frame,\n",
        "        patches_dir,\n",
        "        weather_cols,\n",
        "        mean_img=None,\n",
        "        std_img=None,\n",
        "        mean_weather=None,\n",
        "        std_weather=None\n",
        "    ):\n",
        "        self.df           = data_frame.reset_index(drop=True)\n",
        "        self.patches_dir  = patches_dir\n",
        "        self.weather_cols = weather_cols\n",
        "\n",
        "        # Image normalization stats\n",
        "        self.mean_img = torch.tensor(\n",
        "            mean_img  if mean_img  is not None else [0.0, 0.0, 0.0],\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "        self.std_img = torch.tensor(\n",
        "            std_img   if std_img   is not None else [1.0, 1.0, 1.0],\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "        # Weather normalization stats\n",
        "        num_w = len(weather_cols)\n",
        "        self.mean_weather = torch.tensor(\n",
        "            mean_weather if mean_weather is not None else [0.0]*num_w,\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "        self.std_weather = torch.tensor(\n",
        "            std_weather  if std_weather  is not None else [1.0]*num_w,\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    row = self.df.iloc[idx]\n",
        "    fname = row.get('file_name', row.get('filename')).strip()\n",
        "    tif_path = f\"{self.patches_dir}/{fname}\"\n",
        "\n",
        "    with rasterio.open(tif_path) as src:\n",
        "        patch_data = src.read().astype(np.float32)  # shape (4, H, W)\n",
        "\n",
        "    img    = torch.from_numpy(patch_data[1:4])\n",
        "    target = torch.from_numpy(patch_data[0:1])\n",
        "\n",
        "    img = (img - self.mean_img.view(-1, 1, 1)) / self.std_img.view(-1, 1, 1)\n",
        "\n",
        "    weather_vals = torch.tensor(\n",
        "        row[self.weather_cols].astype(np.float32).values,  # <-- FIX HERE\n",
        "        dtype=torch.float32\n",
        "    )\n",
        "    weather = (weather_vals - self.mean_weather) / self.std_weather\n",
        "\n",
        "    return img, weather, target\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95ywJSOWyS3V"
      },
      "outputs": [],
      "source": [
        "# Fix weather features (force numeric, handle NaNs)\n",
        "for col in weather_feature_cols:\n",
        "    merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce').fillna(0.0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6f_dxjkIi4F"
      },
      "outputs": [],
      "source": [
        "# Example initialization with approximate stats\n",
        "dataset = LSTDataset(\n",
        "    merged_df,\n",
        "    patches_dir,\n",
        "    weather_feature_cols,\n",
        "    mean_img=[0.0, 0.0, 0.0],  # Approximate image means\n",
        "    std_img=[0.3, 0.3, 0.3],   # Approximate image std devs\n",
        "    mean_weather=[0.0] * len(weather_feature_cols),  # Approximate weather means\n",
        "    std_weather=[1.0] * len(weather_feature_cols)    # Approximate weather std devs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGzjleP0isdf",
        "outputId": "fe3bca5c-d302-4dfb-bd76-e5fd574340de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 15050, Validation samples: 3763\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "# Define split sizes\n",
        "total_samples = len(dataset)\n",
        "train_size = int(0.8 * total_samples)\n",
        "val_size = total_samples - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Re‑create train_loader with persistent workers\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True,  # keep workers alive\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW2gujRgi7e9"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ViTLSTModel(nn.Module):\n",
        "    def __init__(self, image_size=64, patch_size=8,\n",
        "                 in_channels=3, embed_dim=128, num_layers=4, num_heads=8, mlp_dim=256, weather_dim=5):\n",
        "        super(ViTLSTModel, self).__init__()\n",
        "        assert image_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (image_size // patch_size) ** 2  # total number of patches\n",
        "\n",
        "        # Patch embedding layer: conv to embed image patches\n",
        "        self.patch_embed = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Positional embeddings for patches + meteorology token\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.n_patches + 1, embed_dim))\n",
        "\n",
        "        # Meteorology feature embedding to token\n",
        "        self.meteor_embed = nn.Linear(weather_dim, embed_dim)\n",
        "\n",
        "        # Transformer encoder (ViT)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
        "                                                  dim_feedforward=mlp_dim, dropout=0.1, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Prediction head: map each patch embedding to patch_size*patch_size output values (for LST)\n",
        "        self.patch_to_pixels = nn.Linear(embed_dim, patch_size * patch_size)\n",
        "\n",
        "        # Initialize weights (optional: follow ViT paper initialization norms if needed)\n",
        "        nn.init.xavier_uniform_(self.patch_embed.weight)\n",
        "        nn.init.normal_(self.pos_embedding, std=0.02)\n",
        "        nn.init.xavier_uniform_(self.meteor_embed.weight)\n",
        "        nn.init.zeros_(self.meteor_embed.bias)\n",
        "        nn.init.xavier_uniform_(self.patch_to_pixels.weight)\n",
        "        nn.init.zeros_(self.patch_to_pixels.bias)\n",
        "\n",
        "    def forward(self, images, weather):\n",
        "        \"\"\"\n",
        "        images: tensor of shape [batch, 3, 64, 64] (input image patches)\n",
        "        weather: tensor of shape [batch, weather_dim] (meteorology features)\n",
        "        \"\"\"\n",
        "        batch_size = images.size(0)\n",
        "        # Patch embedding\n",
        "        patch_tokens = self.patch_embed(images)      # shape: [batch, embed_dim, n_patches_y, n_patches_x]\n",
        "        patch_tokens = patch_tokens.flatten(2).transpose(1, 2)\n",
        "        # Now patch_tokens shape: [batch, n_patches, embed_dim]\n",
        "\n",
        "        # Embed meteorology into token of same dimension\n",
        "        meteor_token = self.meteor_embed(weather)            # shape: [batch, embed_dim]\n",
        "        meteor_token = meteor_token.unsqueeze(1)             # shape: [batch, 1, embed_dim]\n",
        "\n",
        "        # Concatenate image patch tokens with meteorology token\n",
        "        tokens = torch.cat([patch_tokens, meteor_token], dim=1)  # [batch, n_patches + 1, embed_dim]\n",
        "\n",
        "        # Add positional encoding\n",
        "        tokens = tokens + self.pos_embedding[:, :tokens.size(1), :]\n",
        "\n",
        "        # Transformer encoder\n",
        "        x = self.transformer(tokens)  # shape remains [batch, n_patches + 1, embed_dim]\n",
        "\n",
        "        # Separate the output: image patch embeddings and meteorology token (if needed)\n",
        "        # Here we ignore the output meteorology token (last one) for prediction, and use only patch outputs:\n",
        "        patch_outputs = x[:, :-1, :]  # [batch, n_patches, embed_dim]\n",
        "\n",
        "        # Convert each patch embedding to patch pixel predictions\n",
        "        patch_pixels = self.patch_to_pixels(patch_outputs)  # [batch, n_patches, patch_size*patch_size]\n",
        "        # Reshape patch outputs into the spatial layout of the image\n",
        "        # Number of patches per row/col:\n",
        "        patches_per_row = patches_per_col = images.size(2) // self.patch_size  # e.g., 64/8 = 8\n",
        "        # Reshape to [batch, patches_per_row, patches_per_col, patch_size, patch_size]\n",
        "        patch_pixels = patch_pixels.view(batch_size, patches_per_row, patches_per_col,\n",
        "                                         self.patch_size, self.patch_size)\n",
        "        # Permute and reshape to form the full image\n",
        "        patch_pixels = patch_pixels.permute(0, 1, 3, 2, 4).contiguous()  # [batch, patches_per_row, patch_size, patches_per_col, patch_size]\n",
        "        output_image = patch_pixels.view(batch_size, 1,\n",
        "                                         patches_per_row * self.patch_size,\n",
        "                                         patches_per_col * self.patch_size)\n",
        "        return output_image\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WxZn4p9i85M",
        "outputId": "12b4ed0c-a60e-42b3-bf6f-9c3c6251a347"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ViTLSTModel(\n",
            "  (patch_embed): Conv2d(3, 128, kernel_size=(8, 8), stride=(8, 8))\n",
            "  (meteor_embed): Linear(in_features=5, out_features=128, bias=True)\n",
            "  (transformer): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-3): 4 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
            "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (patch_to_pixels): Linear(in_features=128, out_features=64, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ViTLSTModel(image_size=64, patch_size=8,\n",
        "                    in_channels=3, embed_dim=128, num_layers=4, num_heads=8, mlp_dim=256,\n",
        "                    weather_dim=len(weather_feature_cols)).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d82xHUq5R13w"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LoszftfeLY8"
      },
      "outputs": [],
      "source": [
        "#!pip install sympy==1.11.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6z1oFNye_N5"
      },
      "outputs": [],
      "source": [
        "#!pip install sympy==1.8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--07xRvMfYDV"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade --force-reinstall sympy==1.13.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMZkhPloR-u6"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOPhwpthOxV6"
      },
      "outputs": [],
      "source": [
        "print(\"🔥 train_loader batches:\", len(train_loader))\n",
        "print(\"🔥 val_loader batches:  \", len(val_loader))\n",
        "if len(train_loader) == 0:\n",
        "    raise RuntimeError(\"train_loader is empty! No batches to iterate over.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHpoyCX2SDP_",
        "outputId": "ed4cd044-1b12-41d8-cec2-83db8f005357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 [train]: 100%|██████████| 470/470 [5:28:12<00:00, 41.90s/it, loss=720]\n",
            "Epoch 1 [val]: 100%|██████████| 118/118 [2:42:47<00:00, 82.78s/it] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train MSE = 11.5196 | Val MSE = 0.0000\n",
            "Checkpoint saved: /content/drive/MyDrive/checkpoints/model_epoch_1.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 [train]:  13%|█▎        | 61/470 [48:16<5:25:49, 47.80s/it]"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Mount Google Drive if in Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define path to save checkpoints\n",
        "checkpoint_dir = '/content/drive/MyDrive/checkpoints/'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "num_epochs = 10\n",
        "patience = 3\n",
        "best_val_loss = float('inf')\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch} [train]\", leave=True)\n",
        "\n",
        "    for images, weather, targets in train_loader_tqdm:\n",
        "        if torch.isnan(images).any() or torch.isnan(targets).any():\n",
        "            continue\n",
        "\n",
        "        images, weather, targets = images.to(device), weather.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, weather)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # Display current loss in tqdm progress bar\n",
        "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
        "\n",
        "    train_loss /= len(train_dataset)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch} [val]\", leave=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, weather, targets in val_loader_tqdm:\n",
        "            if torch.isnan(images).any() or torch.isnan(targets).any():\n",
        "                continue\n",
        "\n",
        "            images, weather, targets = images.to(device), weather.to(device), targets.to(device)\n",
        "            outputs = model(images, weather)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # Display validation loss in tqdm progress bar\n",
        "            val_loader_tqdm.set_postfix(loss=loss.item())\n",
        "\n",
        "    val_loss /= len(val_dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch}: Train MSE = {train_loss:.4f} | Val MSE = {val_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch}.pth')\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_loss': train_loss,\n",
        "        'val_loss': val_loss,\n",
        "    }, checkpoint_path)\n",
        "\n",
        "    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgZUyk13QqXF"
      },
      "outputs": [],
      "source": [
        "print(\"Train loader workers:\", train_loader.num_workers,\n",
        "      \"| Persistent:\", train_loader.persistent_workers)\n",
        "print(\"Val   loader workers:\",   val_loader.num_workers,\n",
        "      \"| Persistent:\", val_loader.persistent_workers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgUIozw7Nagx"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Grab one batch\n",
        "images, weather, targets = next(iter(train_loader))\n",
        "images, weather, targets = images.to(device), weather.to(device), targets.to(device)\n",
        "\n",
        "# Warm up CUDA (if you haven’t already)\n",
        "_ = model(images, weather)\n",
        "\n",
        "# Time one training step\n",
        "torch.cuda.synchronize()          # ensure all queued work is done\n",
        "start = time.time()\n",
        "\n",
        "optimizer.zero_grad()\n",
        "outputs = model(images, weather)\n",
        "loss    = criterion(outputs, targets)\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "elapsed = time.time() - start\n",
        "print(f\"One batch took {elapsed*1000:.1f} ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39dxuTRXNt57"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Pick one sample index (e.g. 0)\n",
        "idx = 0\n",
        "\n",
        "# Time pure __getitem__ outside DataLoader\n",
        "start = time.time()\n",
        "img, weather, target = train_dataset.dataset[idx] if hasattr(train_dataset, 'dataset') else dataset[idx]\n",
        "elapsed = time.time() - start\n",
        "print(f\"Single __getitem__ took {elapsed:.3f} s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GesvGOi8jDnF"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "save_path = os.path.join(base_path, \"vit_lst_model.pth\")\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"Model checkpoint saved at {save_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}